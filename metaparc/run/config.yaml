wandb:
  project: General Physics Model
  entity: wsnr-florian
  id: tokenizer_overlap
  tags:
    - TestRun
  log_model: gradients # log :all", "parameters" or "gradients"
  log_interval: 100
  notes: "Small model - overlapping tokenizer and detokenizer"

logging:
  log_dir: /Users/zsa8rk/Coding/MetaPARC/logs
  log_file: null #log.txt
  log_level: INFO

model:
  architecture:
    name: "Spatial-Temporal Transformer"
    backbone: "spatial temporal attention"
    normalization: "RevIN (T,H,W)"
    activation: GELU

  transformer:
    input_channels: 5 # number of input fields (usually 5 (pressure, density, temperature, vel_x, vel_y))
    # number of hidden channels across all layers (must be divisible by 6 and by num_heads)
    hidden_channels: 126
    
    mlp_dim: 378 # number of hidden channels in the MLP
    num_heads: 2 # number of attention heads
    dropout: 0.0 # dropout rate
    pos_enc_mode: absolute # positional encoding mode (usually rope or absolute)
    patch_size: [2, 16, 16] # patch size (usually 4x16x16)
    num_layers: 2 # number of attention layers
    stochastic_depth_rate: 0.0 # stochastic depth rate
  tokenizer:
    tokenizer_mode: linear # tokenizer mode (usually linear or conv_net)
    detokenizer_mode: linear # detokenizer mode (usually linear or conv_net)
    tokenizer_overlap: 4 # pixels overlap between patches, only used for linear tokenizer
    detokenizer_overlap: 4 # pixels overlap between patches, only used for linear detokenizer
    tokenizer_net_channels: null # number of channels in the tokenizer conv_net
    detokenizer_net_channels: null # number of channels in the detokenizer conv_net

training:
  batch_size: 64
  epochs: 10
  num_workers: 6
  grad_clip: 1.0 # gradient clipping
  optimizer:
    name: Adam
    betas: [0.9, 0.999]
    learning_rate: 0.0001
    weight_decay: 0.0000
    # name: AdamW
    # learning_rate: 0.001
    # weight_decay: 0.01
    # betas: [0.9, 0.999]
    # name: DAdaptAdam
    # betas: [0.9, 0.999]
    # weight_decay: 0.01
    # name: DAdaptAdamW
    # betas: [0.9, 0.999]
    # weight_decay: 0.1
  criterion:
    name: MSELoss

  lr_scheduler:
    schedulers:
      LinearLR: # linear warmup scheduler
        start_factor: 0.001
        end_factor: 1.0
        epochs: 1 # number of training epochs for linear warmup
      # CosineAnnealingWarmRestarts: # cosine annealing scheduler
      #   T_0: 10 # number epochs for one cosine annealing cycle
      #   T_mult: 2 # multiplicative factor for the number of epochs in each cycle
      #   min_lr: 0.0001
      # CosineAnnealingLR: # cosine annealing scheduler
      #   T_max: 2 # Number of epochs for one cosine annealing cycle
      #   min_lr: 0.0001
data:
  length_limit: null # limit the number of samples per epoch, set to null for no limit
  dt_stride: 1 # take every dt_stride-th timestep
  n_steps_input: 4 # number of input timesteps
  n_steps_output: 1 # number of target timesteps
  out_shape: [128, 64] # output shape (usually)

  data_dir: /Users/zsa8rk/Coding/MetaPARC/data/datasets
  datasets:
    # - cylinder_sym_flow_water
    # - cylinder_pipe_flow_water
    # - object_periodic_flow_water
    # - object_sym_flow_water
    # - object_sym_flow_air

    # - heated_object_pipe_flow_air

    - turbulent_radiative_layer_2D
    # - rayleigh_benard
    # - shear_flow
    # - euler
