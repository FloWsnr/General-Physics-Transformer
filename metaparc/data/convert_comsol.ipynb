{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert COMSOL data to PARC data\n",
    "\n",
    "This notebook is used to convert COMSOL simulation data into tensors suitable for deep learning.\n",
    "It aggregates the data from a parametric sweep into tensors of shape (N_timesteps, X, Y, N_features)\n",
    "\n",
    "The input data is in the form of a csv file with the following columns:\n",
    "x0 | y0 | t1 | t2 | t3 | ... | tN\n",
    "x0 | y1 | t1 | t2 | t3 | ... | tN\n",
    "x0 | y2 | t1 | t2 | t3 | ... | tN\n",
    "...\n",
    "\n",
    "where t1, t2, t3, ... are the time steps.\n",
    "\n",
    "The output data is then stored as hdf5 files with the structure of the-well format.\n",
    "In our case, theta and p_cap are parameters while the strucID is a starting condition,\n",
    "i.e. the trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple, Any\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "def search_comsol_data(data_path: Path, params: list[str]) -> Dict[Tuple[int, int, float], Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Search through velocities, pressure, and phase_boundary folders to find and group\n",
    "    files with matching parameter combinations.\n",
    "\n",
    "    Args:\n",
    "        data_path (Path): Base path containing the subdirectories\n",
    "\n",
    "        params (list[str]): List of parameters to extract from the filenames\n",
    "\n",
    "    Returns:\n",
    "        Dict: Dictionary with parameter combinations as keys and file information as values\n",
    "    \"\"\"\n",
    "    # Define the subdirectories to search\n",
    "    subdirs = [\"velocities\", \"pressure\", \"phase_boundary\"]\n",
    "    data = {}\n",
    "\n",
    "    # Function to extract parameters from filename\n",
    "    def extract_params(filename: str, params: list[str]) -> tuple:\n",
    "        param1 = re.search(rf\"{params[0]}_(\\d+)\", filename).group(1)\n",
    "        param2 = re.search(rf\"{params[1]}_(\\d+)\", filename).group(1)\n",
    "        param3 = re.search(rf\"{params[2]}_(\\d+\\.?\\d*)\", filename).group(1)\n",
    "\n",
    "        # Transform theta from radians to degrees\n",
    "        param3 = float(param3)\n",
    "        param3 = np.rad2deg(param3)\n",
    "        # round theta to integer\n",
    "        param3 = int(np.round(param3))\n",
    "\n",
    "        return (int(param1), int(param2), param3)\n",
    "\n",
    "    # Search through each subdirectory\n",
    "    for subdir in subdirs:\n",
    "        subdir_path = data_path / subdir\n",
    "        if not subdir_path.exists():\n",
    "            print(f\"Warning: Directory {subdir} not found\")\n",
    "            continue\n",
    "\n",
    "        files = subdir_path.glob(\"*.csv\")\n",
    "        for file in files:\n",
    "            # Extract parameters\n",
    "            param1, param2, param3 = extract_params(file.stem, params)\n",
    "\n",
    "            # Create parameter combination key, only include p_cap and theta\n",
    "            # strucID is not included since we want to aggregate over all strucIDs\n",
    "            param_key = (param2, param3)\n",
    "\n",
    "            # Initialize nested dictionaries if they don't exist\n",
    "            if param_key not in data:\n",
    "                data[param_key] = {\n",
    "                    \"vel_u\": {},\n",
    "                    \"vel_v\": {},\n",
    "                    \"pressure\": {},\n",
    "                    \"phase_boundary\": {},\n",
    "                }\n",
    "\n",
    "            # Add file path under appropriate category\n",
    "            if \"vel_u\" in str(file):\n",
    "                data[param_key][\"vel_u\"][param1] = file\n",
    "            elif \"vel_v\" in str(file):\n",
    "                data[param_key][\"vel_v\"][param1] = file\n",
    "            elif \"pressure\" in str(file):\n",
    "                data[param_key][\"pressure\"][param1] = file\n",
    "            elif \"phase_boundary\" in str(file):\n",
    "                data[param_key][\"phase_boundary\"][param1] = file\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_comsol_csv(file_path: Path) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load COMSOL CSV data and reshape it into (time, x, y) format.\n",
    "    \"\"\"\n",
    "    # Read CSV and sort by x then y coordinates\n",
    "    df = pd.read_csv(file_path, sep=\";\", dtype=np.float32, header=None)\n",
    "    df = df.sort_values(by=[df.columns[0], df.columns[1]]).reset_index(drop=True)\n",
    "\n",
    "    # Get coordinate information\n",
    "    x_coords = df[0].unique()\n",
    "    y_coords = df[1].unique()\n",
    "\n",
    "    # Calculate expected grid size and validate\n",
    "    grid_size = len(x_coords) * len(y_coords)\n",
    "    if len(df) != grid_size:\n",
    "        raise ValueError(\n",
    "            f\"Data grid mismatch: {len(df)} points vs expected {grid_size} ({len(x_coords)}x{len(y_coords)})\"\n",
    "        )\n",
    "\n",
    "    # Reshape directly to (x, y, time) then transpose to (time, y, x)\n",
    "    time_steps = df.columns[2:]\n",
    "    time_data = df[time_steps].values\n",
    "    data = time_data.reshape(len(x_coords), len(y_coords), len(time_steps))\n",
    "\n",
    "    return data.transpose(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hdf5_dataset(output_path: Path, param_key: tuple, data_dict: dict):\n",
    "    \"\"\"\n",
    "    Create HDF5 file with the specified format.\n",
    "    \"\"\"\n",
    "    p_cap, theta = param_key\n",
    "    filename = f\"porous_twophase_flow_p_cap_{p_cap}_theta_{theta}.hdf5\"\n",
    "\n",
    "    with h5py.File(output_path / filename, \"w\") as f:\n",
    "        # Root attributes\n",
    "        f.attrs[\"simulation_parameters\"] = [\"p_cap\", \"theta\"]\n",
    "        f.attrs[\"p_cap\"] = p_cap\n",
    "        f.attrs[\"theta\"] = theta\n",
    "        f.attrs[\"dataset_name\"] = \"COMSOL_TwoPhaseFlow\"\n",
    "        f.attrs[\"grid_type\"] = \"cartesian\"\n",
    "        f.attrs[\"n_spatial_dims\"] = 2\n",
    "        f.attrs[\"n_trajectories\"] = 1  # Number of strucIDs\n",
    "\n",
    "        # Load data from first file to get dimensions\n",
    "        first_file = next(iter(data_dict[\"files\"].values()))\n",
    "        x_coords, y_coords, time_coords, _ = load_comsol_csv(first_file)\n",
    "\n",
    "        # Create dimensions group\n",
    "        dims = f.create_group(\"dimensions\")\n",
    "        dims.attrs[\"spatial_dims\"] = [\"x\", \"y\"]\n",
    "\n",
    "        time_dset = dims.create_dataset(\"time\", data=time_coords)\n",
    "        time_dset.attrs[\"sample_varying\"] = False\n",
    "\n",
    "        x_dset = dims.create_dataset(\"x\", data=x_coords)\n",
    "        x_dset.attrs[\"sample_varying\"] = False\n",
    "        x_dset.attrs[\"time_varying\"] = False\n",
    "\n",
    "        y_dset = dims.create_dataset(\"y\", data=y_coords)\n",
    "        y_dset.attrs[\"sample_varying\"] = False\n",
    "        y_dset.attrs[\"time_varying\"] = False\n",
    "\n",
    "        # Create boundary conditions group\n",
    "        bc = f.create_group(\"boundary_conditions\")\n",
    "        x_bc = bc.create_group(\"X_boundary\")\n",
    "        x_bc.attrs[\"associated_dims\"] = [\"x\"]\n",
    "        x_bc.attrs[\"associated_fields\"] = []\n",
    "        x_bc.attrs[\"bc_type\"] = \"wall\"\n",
    "        x_bc.attrs[\"sample_varying\"] = False\n",
    "        x_bc.attrs[\"time_varying\"] = False\n",
    "\n",
    "        # Create mask dataset (assuming periodic in x)\n",
    "        mask = np.zeros_like(x_coords, dtype=bool)\n",
    "        mask[0] = mask[-1] = True\n",
    "        x_bc.create_dataset(\"mask\", data=mask)\n",
    "        x_bc.create_dataset(\"values\", data=np.array([]))\n",
    "\n",
    "        # Create scalars group\n",
    "        scalars = f.create_group(\"scalars\")\n",
    "        scalars.attrs[\"field_names\"] = [\"p_cap\", \"theta\"]\n",
    "\n",
    "        p_cap_dset = scalars.create_dataset(\"p_cap\", data=p_cap)\n",
    "        p_cap_dset.attrs[\"sample_varying\"] = False\n",
    "        p_cap_dset.attrs[\"time_varying\"] = False\n",
    "\n",
    "        theta_dset = scalars.create_dataset(\"theta\", data=theta)\n",
    "        theta_dset.attrs[\"sample_varying\"] = False\n",
    "        theta_dset.attrs[\"time_varying\"] = False\n",
    "\n",
    "        # Create t0_fields group for pressure\n",
    "        t0_fields = f.create_group(\"t0_fields\")\n",
    "        t0_fields.attrs[\"field_names\"] = [\"pressure\"]\n",
    "\n",
    "        # Load and store pressure field\n",
    "        _, _, _, pressure_data = load_comsol_csv(data_dict[\"files\"][\"pressure\"])\n",
    "        pressure_dset = t0_fields.create_dataset(\n",
    "            \"pressure\", data=pressure_data[np.newaxis, ...]\n",
    "        )\n",
    "        pressure_dset.attrs[\"dim_varying\"] = [True, True]\n",
    "        pressure_dset.attrs[\"sample_varying\"] = True\n",
    "        pressure_dset.attrs[\"time_varying\"] = True\n",
    "\n",
    "        # Create t1_fields group for velocities\n",
    "        t1_fields = f.create_group(\"t1_fields\")\n",
    "        t1_fields.attrs[\"field_names\"] = [\"velocity\"]\n",
    "\n",
    "        # Load velocity components\n",
    "        _, _, _, vel_u = load_comsol_csv(data_dict[\"files\"][\"vel_u\"])\n",
    "        _, _, _, vel_v = load_comsol_csv(data_dict[\"files\"][\"vel_v\"])\n",
    "\n",
    "        # Combine velocity components and store\n",
    "        velocity = np.stack([vel_u, vel_v], axis=-1)\n",
    "        velocity_dset = t1_fields.create_dataset(\n",
    "            \"velocity\", data=velocity[np.newaxis, ...]\n",
    "        )\n",
    "        velocity_dset.attrs[\"dim_varying\"] = [True, True]\n",
    "        velocity_dset.attrs[\"sample_varying\"] = True\n",
    "        velocity_dset.attrs[\"time_varying\"] = True\n",
    "\n",
    "        # Create empty t2_fields group\n",
    "        t2_fields = f.create_group(\"t2_fields\")\n",
    "        t2_fields.attrs[\"field_names\"] = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 401, 256, 128)\n",
      "(2, 401, 256, 128)\n",
      "(2, 401, 256, 128)\n",
      "(2, 401, 256, 128)\n",
      "(2, 401, 256, 128)\n",
      "(2, 401, 256, 128)\n",
      "(2, 401, 256, 128)\n",
      "(2, 401, 256, 128)\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "raw_data_path = Path(\"/home/flwi01/Coding/MetaPARC/data/raw_comsol\")\n",
    "data_path = Path(\"/home/flwi01/Coding/MetaPARC/data/tasks/porous_twophase_flow\")\n",
    "data_path.mkdir(parents=True, exist_ok=True)\n",
    "param_names = [\"strucID\", \"p_cap\", \"theta\"]\n",
    "\n",
    "data_files: dict[tuple[int, int], dict[str, dict[int, Path]]] = search_comsol_data(raw_data_path, param_names)\n",
    "\n",
    "for params_key, features in data_files.items():\n",
    "    data = {}\n",
    "    data[\"params\"] = params_key\n",
    "    for feature_name, trajectories in features.items():\n",
    "        trajectory_data = []\n",
    "        for trajectory_id, file_path in trajectories.items():\n",
    "            feature_array = load_comsol_csv(file_path)\n",
    "            trajectory_data.append(feature_array)\n",
    "\n",
    "\n",
    "        trajectory_data = np.stack(trajectory_data, axis=0) # (n_trajectories, n_timesteps, x, y)\n",
    "        data[feature_name] = trajectory_data\n",
    "\n",
    "    # Add metadata from the last trajectory\n",
    "    data[\"n_trajectories\"] = trajectory_data.shape[0]\n",
    "    data[\"n_timesteps\"] = trajectory_data.shape[1]\n",
    "    data[\"x_coords\"] = trajectory_data.shape[2]\n",
    "    data[\"y_coords\"] = trajectory_data.shape[3]\n",
    "\n",
    "    print(data[\"vel_u\"].shape)\n",
    "    print(data[\"vel_v\"].shape)\n",
    "    print(data[\"pressure\"].shape)\n",
    "    print(data[\"phase_boundary\"].shape)\n",
    "\n",
    "    # join vel_u and vel_v\n",
    "    data[\"velocity\"] = np.stack([data[\"vel_u\"], data[\"vel_v\"]], axis=-1)\n",
    "    # remove vel_u and vel_v\n",
    "    del data[\"vel_u\"]\n",
    "    del data[\"vel_v\"]\n",
    "\n",
    "    # data_path.mkdir(parents=True, exist_ok=True)\n",
    "    # np.save(data_path / f\"p_cap_{params_key[1]}_theta_{params_key[2]}.npy\", data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/flwi01/Coding/MetaPARC/data/tasks/porous_twophase_flow/strucID_0_p_cap_0_theta_110.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Plot 4 data timesteps to check if correct\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m data_files \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_path\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrucID_0_p_cap_0_theta_110.npy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     11\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/parc/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py:459\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    457\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    460\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/flwi01/Coding/MetaPARC/data/tasks/porous_twophase_flow/strucID_0_p_cap_0_theta_110.npy'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot 4 data timesteps to check if correct\n",
    "\n",
    "data_files = np.load(\n",
    "    data_path\n",
    "    / \"strucID_0_p_cap_0_theta_110.npy\"\n",
    ")\n",
    "feature = 3\n",
    "\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "axs[0, 0].imshow(data_files[0, :, :, feature])\n",
    "axs[0, 1].imshow(data_files[100, :, :, feature])\n",
    "axs[1, 0].imshow(data_files[200, :, :, feature])\n",
    "axs[1, 1].imshow(data_files[300, :, :, feature])\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
