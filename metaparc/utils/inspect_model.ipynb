{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect a given model\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import yaml\n",
    "from metaparc.model.transformer.model import get_model\n",
    "\n",
    "def inspect_model(model: nn.Module):\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "\n",
    "    # Print the number of parameters for each layer\n",
    "    for name, module in model.named_modules():\n",
    "        print(f\"{name}: {sum(p.numel() for p in module.parameters())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PhysicsTransformer(\n",
      "  (pos_encodings): RotaryPositionalEmbedding()\n",
      "  (attention_blocks): ModuleList(\n",
      "    (0-11): 12 x AttentionBlock(\n",
      "      (attention): SpatioTemporalAttention(\n",
      "        (pe): RotaryPositionalEmbedding()\n",
      "        (to_qkv): Conv3d(384, 1152, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "        (attention): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=384, out_features=384, bias=True)\n",
      "        )\n",
      "      )\n",
      "      (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): MLP(\n",
      "        (mlp): Sequential(\n",
      "          (0): Conv3d(384, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=valid)\n",
      "          (1): GELU(approximate='none')\n",
      "          (2): Conv3d(384, 384, kernel_size=(1, 1, 1), stride=(1, 1, 1), padding=valid)\n",
      "          (3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (tokenizer): SpatioTemporalTokenization(\n",
      "    (token_net): Sequential(\n",
      "      (0): Conv3d(5, 192, kernel_size=(2, 4, 4), stride=(2, 4, 4), padding=valid, bias=False)\n",
      "      (1): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): Conv3d(192, 384, kernel_size=(2, 4, 4), stride=(2, 4, 4), padding=valid, bias=False)\n",
      "      (4): InstanceNorm3d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (5): GELU(approximate='none')\n",
      "    )\n",
      "  )\n",
      "  (detokenizer): SpatioTemporalDetokenization(\n",
      "    (de_patch_net): Sequential(\n",
      "      (0): ConvTranspose3d(384, 192, kernel_size=(2, 4, 4), stride=(2, 4, 4), bias=False)\n",
      "      (1): InstanceNorm3d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (2): GELU(approximate='none')\n",
      "      (3): ConvTranspose3d(192, 5, kernel_size=(2, 4, 4), stride=(2, 4, 4), bias=False)\n",
      "      (4): InstanceNorm3d(5, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "      (5): GELU(approximate='none')\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "config_path = Path(\"/Users/zsa8rk/Coding/MetaPARC/metaparc/run/config.yaml\")\n",
    "\n",
    "with open(config_path, \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "model_config = config[\"model\"]\n",
    "\n",
    "\n",
    "model = get_model(model_config)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 20,752,906 (20.75M)\n",
      ": 20752906\n",
      "pos_encodings: 0\n",
      "attention_blocks: 15971328\n",
      "attention_blocks.0: 1330944\n",
      "attention_blocks.0.attention: 1033728\n",
      "attention_blocks.0.attention.to_qkv: 442368\n",
      "attention_blocks.0.attention.attention: 591360\n",
      "attention_blocks.0.attention.attention.out_proj: 147840\n",
      "attention_blocks.0.norm1: 768\n",
      "attention_blocks.0.norm2: 768\n",
      "attention_blocks.0.mlp: 295680\n",
      "attention_blocks.0.mlp.mlp: 295680\n",
      "attention_blocks.0.mlp.mlp.0: 147840\n",
      "attention_blocks.0.mlp.mlp.1: 0\n",
      "attention_blocks.0.mlp.mlp.2: 147840\n",
      "attention_blocks.0.mlp.mlp.3: 0\n",
      "attention_blocks.1: 1330944\n",
      "attention_blocks.1.attention: 1033728\n",
      "attention_blocks.1.attention.to_qkv: 442368\n",
      "attention_blocks.1.attention.attention: 591360\n",
      "attention_blocks.1.attention.attention.out_proj: 147840\n",
      "attention_blocks.1.norm1: 768\n",
      "attention_blocks.1.norm2: 768\n",
      "attention_blocks.1.mlp: 295680\n",
      "attention_blocks.1.mlp.mlp: 295680\n",
      "attention_blocks.1.mlp.mlp.0: 147840\n",
      "attention_blocks.1.mlp.mlp.1: 0\n",
      "attention_blocks.1.mlp.mlp.2: 147840\n",
      "attention_blocks.1.mlp.mlp.3: 0\n",
      "attention_blocks.2: 1330944\n",
      "attention_blocks.2.attention: 1033728\n",
      "attention_blocks.2.attention.to_qkv: 442368\n",
      "attention_blocks.2.attention.attention: 591360\n",
      "attention_blocks.2.attention.attention.out_proj: 147840\n",
      "attention_blocks.2.norm1: 768\n",
      "attention_blocks.2.norm2: 768\n",
      "attention_blocks.2.mlp: 295680\n",
      "attention_blocks.2.mlp.mlp: 295680\n",
      "attention_blocks.2.mlp.mlp.0: 147840\n",
      "attention_blocks.2.mlp.mlp.1: 0\n",
      "attention_blocks.2.mlp.mlp.2: 147840\n",
      "attention_blocks.2.mlp.mlp.3: 0\n",
      "attention_blocks.3: 1330944\n",
      "attention_blocks.3.attention: 1033728\n",
      "attention_blocks.3.attention.to_qkv: 442368\n",
      "attention_blocks.3.attention.attention: 591360\n",
      "attention_blocks.3.attention.attention.out_proj: 147840\n",
      "attention_blocks.3.norm1: 768\n",
      "attention_blocks.3.norm2: 768\n",
      "attention_blocks.3.mlp: 295680\n",
      "attention_blocks.3.mlp.mlp: 295680\n",
      "attention_blocks.3.mlp.mlp.0: 147840\n",
      "attention_blocks.3.mlp.mlp.1: 0\n",
      "attention_blocks.3.mlp.mlp.2: 147840\n",
      "attention_blocks.3.mlp.mlp.3: 0\n",
      "attention_blocks.4: 1330944\n",
      "attention_blocks.4.attention: 1033728\n",
      "attention_blocks.4.attention.to_qkv: 442368\n",
      "attention_blocks.4.attention.attention: 591360\n",
      "attention_blocks.4.attention.attention.out_proj: 147840\n",
      "attention_blocks.4.norm1: 768\n",
      "attention_blocks.4.norm2: 768\n",
      "attention_blocks.4.mlp: 295680\n",
      "attention_blocks.4.mlp.mlp: 295680\n",
      "attention_blocks.4.mlp.mlp.0: 147840\n",
      "attention_blocks.4.mlp.mlp.1: 0\n",
      "attention_blocks.4.mlp.mlp.2: 147840\n",
      "attention_blocks.4.mlp.mlp.3: 0\n",
      "attention_blocks.5: 1330944\n",
      "attention_blocks.5.attention: 1033728\n",
      "attention_blocks.5.attention.to_qkv: 442368\n",
      "attention_blocks.5.attention.attention: 591360\n",
      "attention_blocks.5.attention.attention.out_proj: 147840\n",
      "attention_blocks.5.norm1: 768\n",
      "attention_blocks.5.norm2: 768\n",
      "attention_blocks.5.mlp: 295680\n",
      "attention_blocks.5.mlp.mlp: 295680\n",
      "attention_blocks.5.mlp.mlp.0: 147840\n",
      "attention_blocks.5.mlp.mlp.1: 0\n",
      "attention_blocks.5.mlp.mlp.2: 147840\n",
      "attention_blocks.5.mlp.mlp.3: 0\n",
      "attention_blocks.6: 1330944\n",
      "attention_blocks.6.attention: 1033728\n",
      "attention_blocks.6.attention.to_qkv: 442368\n",
      "attention_blocks.6.attention.attention: 591360\n",
      "attention_blocks.6.attention.attention.out_proj: 147840\n",
      "attention_blocks.6.norm1: 768\n",
      "attention_blocks.6.norm2: 768\n",
      "attention_blocks.6.mlp: 295680\n",
      "attention_blocks.6.mlp.mlp: 295680\n",
      "attention_blocks.6.mlp.mlp.0: 147840\n",
      "attention_blocks.6.mlp.mlp.1: 0\n",
      "attention_blocks.6.mlp.mlp.2: 147840\n",
      "attention_blocks.6.mlp.mlp.3: 0\n",
      "attention_blocks.7: 1330944\n",
      "attention_blocks.7.attention: 1033728\n",
      "attention_blocks.7.attention.to_qkv: 442368\n",
      "attention_blocks.7.attention.attention: 591360\n",
      "attention_blocks.7.attention.attention.out_proj: 147840\n",
      "attention_blocks.7.norm1: 768\n",
      "attention_blocks.7.norm2: 768\n",
      "attention_blocks.7.mlp: 295680\n",
      "attention_blocks.7.mlp.mlp: 295680\n",
      "attention_blocks.7.mlp.mlp.0: 147840\n",
      "attention_blocks.7.mlp.mlp.1: 0\n",
      "attention_blocks.7.mlp.mlp.2: 147840\n",
      "attention_blocks.7.mlp.mlp.3: 0\n",
      "attention_blocks.8: 1330944\n",
      "attention_blocks.8.attention: 1033728\n",
      "attention_blocks.8.attention.to_qkv: 442368\n",
      "attention_blocks.8.attention.attention: 591360\n",
      "attention_blocks.8.attention.attention.out_proj: 147840\n",
      "attention_blocks.8.norm1: 768\n",
      "attention_blocks.8.norm2: 768\n",
      "attention_blocks.8.mlp: 295680\n",
      "attention_blocks.8.mlp.mlp: 295680\n",
      "attention_blocks.8.mlp.mlp.0: 147840\n",
      "attention_blocks.8.mlp.mlp.1: 0\n",
      "attention_blocks.8.mlp.mlp.2: 147840\n",
      "attention_blocks.8.mlp.mlp.3: 0\n",
      "attention_blocks.9: 1330944\n",
      "attention_blocks.9.attention: 1033728\n",
      "attention_blocks.9.attention.to_qkv: 442368\n",
      "attention_blocks.9.attention.attention: 591360\n",
      "attention_blocks.9.attention.attention.out_proj: 147840\n",
      "attention_blocks.9.norm1: 768\n",
      "attention_blocks.9.norm2: 768\n",
      "attention_blocks.9.mlp: 295680\n",
      "attention_blocks.9.mlp.mlp: 295680\n",
      "attention_blocks.9.mlp.mlp.0: 147840\n",
      "attention_blocks.9.mlp.mlp.1: 0\n",
      "attention_blocks.9.mlp.mlp.2: 147840\n",
      "attention_blocks.9.mlp.mlp.3: 0\n",
      "attention_blocks.10: 1330944\n",
      "attention_blocks.10.attention: 1033728\n",
      "attention_blocks.10.attention.to_qkv: 442368\n",
      "attention_blocks.10.attention.attention: 591360\n",
      "attention_blocks.10.attention.attention.out_proj: 147840\n",
      "attention_blocks.10.norm1: 768\n",
      "attention_blocks.10.norm2: 768\n",
      "attention_blocks.10.mlp: 295680\n",
      "attention_blocks.10.mlp.mlp: 295680\n",
      "attention_blocks.10.mlp.mlp.0: 147840\n",
      "attention_blocks.10.mlp.mlp.1: 0\n",
      "attention_blocks.10.mlp.mlp.2: 147840\n",
      "attention_blocks.10.mlp.mlp.3: 0\n",
      "attention_blocks.11: 1330944\n",
      "attention_blocks.11.attention: 1033728\n",
      "attention_blocks.11.attention.to_qkv: 442368\n",
      "attention_blocks.11.attention.attention: 591360\n",
      "attention_blocks.11.attention.attention.out_proj: 147840\n",
      "attention_blocks.11.norm1: 768\n",
      "attention_blocks.11.norm2: 768\n",
      "attention_blocks.11.mlp: 295680\n",
      "attention_blocks.11.mlp.mlp: 295680\n",
      "attention_blocks.11.mlp.mlp.0: 147840\n",
      "attention_blocks.11.mlp.mlp.1: 0\n",
      "attention_blocks.11.mlp.mlp.2: 147840\n",
      "attention_blocks.11.mlp.mlp.3: 0\n",
      "tokenizer: 2391168\n",
      "tokenizer.token_net: 2391168\n",
      "tokenizer.token_net.0: 30720\n",
      "tokenizer.token_net.1: 384\n",
      "tokenizer.token_net.2: 0\n",
      "tokenizer.token_net.3: 2359296\n",
      "tokenizer.token_net.4: 768\n",
      "tokenizer.token_net.5: 0\n",
      "detokenizer: 2390410\n",
      "detokenizer.de_patch_net: 2390410\n",
      "detokenizer.de_patch_net.0: 2359296\n",
      "detokenizer.de_patch_net.1: 384\n",
      "detokenizer.de_patch_net.2: 0\n",
      "detokenizer.de_patch_net.3: 30720\n",
      "detokenizer.de_patch_net.4: 10\n",
      "detokenizer.de_patch_net.5: 0\n"
     ]
    }
   ],
   "source": [
    "inspect_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
