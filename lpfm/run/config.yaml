wandb:
  project: Large-Physics-Foundation-Model
  entity: wsnr-florian
  id: test-worker-benchmark
  tags:
    - LPFM-Ti
    - TestRun
  log_model: gradients # log :all", "parameters" or "gradients" or null
  log_interval: 10
  notes: "Test how many workers are optimal"

logging:
  log_dir: /Users/zsa8rk/Coding/Large-Physics-Foundation-Model/logs
  log_file: null #log.txt
  log_level: INFO

model:
  architecture:
    name: "Spatial-Temporal Transformer"
    backbone: "spatial temporal attention"
    normalization: "RevIN (T,H,W)"
    activation: GELU
    patches: linear
  
  transformer:
    input_channels: 5 # number of input fields (usually 5 (pressure, density, temperature, vel_x, vel_y))
    # number of hidden channels across all layers (must be divisible by 6 and by num_heads)
    model_size: LPFM_Ti # model config (LPFM_Ti, LPFM_S, LPFM_M, LPFM_L)
    dropout: 0.0 # dropout rate
    pos_enc_mode: absolute # positional encoding mode (usually rope or absolute)
    patch_size: [4, 16, 16] # patch size (usually 4x16x16)
    stochastic_depth_rate: 0.0 # stochastic depth rate
    use_derivatives: true # whether to use derivatives in the model
  tokenizer:
    tokenizer_mode: linear # tokenizer mode (usually linear or conv_net)
    detokenizer_mode: linear # detokenizer mode (usually linear or conv_net)
    tokenizer_overlap: 2 # pixels overlap between patches, only used for linear tokenizer
    detokenizer_overlap: 2 # pixels overlap between patches, only used for linear detokenizer
    detokenizer_squash_time: false # squash time dimension in detokenizer
    tokenizer_net_channels: null # number of channels in the tokenizer conv_net
    detokenizer_net_channels: null # number of channels in the detokenizer conv_net

training:
  compile: true
  tf32: true # use tf32 for faster matrix multiplications
  amp: true # use automatic mixed precision training
  seed: 42
  batch_size: 64 # batch size per GPU
  epochs: 2
  num_workers: 4 # number of workers for dataloader per GPU
  grad_clip: 1.0 # gradient clipping
  optimizer:
    # name: Adam
    # betas: [0.9, 0.999]
    # learning_rate: 0.0001
    # weight_decay: 0.0000
    name: AdamW
    learning_rate: 1e-4
    weight_decay: 0.01
    betas: [0.9, 0.999]
    # name: DAdaptAdam
    # betas: [0.9, 0.999]
    # weight_decay: 0.01
    # name: DAdaptAdamW
    # betas: [0.9, 0.999]
    # weight_decay: 0.1
  criterion: NMSE # MSE or NMSE or MAE

  lr_scheduler:
    schedulers:
      LinearLR: # linear warmup scheduler
        start_factor: 0.001
        end_factor: 1.0
        epochs: 1 # number of training epochs for linear warmup
      # CosineAnnealingWarmRestarts: # cosine annealing scheduler
      #   T_0: 9 # number epochs for one cosine annealing cycle
      #   T_mult: 2 # multiplicative factor for the number of epochs in each cycle
      #   min_lr: 0.0001
      CosineAnnealingLR: # cosine annealing scheduler
        T_max: -1 # Number of epochs for one cosine annealing cycle, if -1, use all remaining epochs
        min_lr: 1e-6

data:
  max_samples_per_ds: null # limit the number of samples per epoch, set to null for no limit
  dt_stride: [1,8] # take every dt_stride-th timestep, if list, randomize between the values
  n_steps_input: 16 # number of input timesteps
  n_steps_output: 1 # number of target timesteps
  out_shape: [256, 128] # output shape (usually)

  data_dir: /Users/zsa8rk/Coding/Large-Physics-Foundation-Model/data/datasets
  datasets:
    # - cylinder_sym_flow_water
    # - cylinder_pipe_flow_water
    # - object_periodic_flow_water
    # - object_sym_flow_water
    # - object_sym_flow_air

    # - heated_object_pipe_flow_air
    # - cooled_object_pipe_flow_air

    # - rayleigh_benard
    # - shear_flow
    # - euler_multi_quadrants_periodicBC

    # For testing
    - turbulent_radiative_layer_2D
