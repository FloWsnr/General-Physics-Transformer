wandb:
  project: Large-Physics-Foundation-Model
  entity: wsnr-florian
  id: med-more-timesteps
  tags:
    - TestRun
    - LPFM-M
  log_model: gradients # log :all", "parameters" or "gradients"
  log_interval: 100
  notes: "Medium model - test run with more timesteps"

logging:
  log_dir: /Users/zsa8rk/Coding/Large-Physics-Foundation-Model/logs
  log_file: null #log.txt
  log_level: INFO

model:
  architecture:
    name: "Spatial-Temporal Transformer"
    backbone: "spatial temporal attention"
    normalization: "RevIN (T,H,W)"
    activation: GELU
    patches: linear

  transformer:
    input_channels: 5 # number of input fields (usually 5 (pressure, density, temperature, vel_x, vel_y))
    # number of hidden channels across all layers (must be divisible by 6 and by num_heads)
    hidden_channels: 384
    
    mlp_dim: 1536 # number of hidden channels in the MLP
    num_heads: 12 # number of attention heads
    dropout: 0.0 # dropout rate
    pos_enc_mode: absolute # positional encoding mode (usually rope or absolute)
    patch_size: [4, 16, 16] # patch size (usually 4x16x16)
    num_layers: 12 # number of attention layers
    stochastic_depth_rate: 0.0 # stochastic depth rate
  tokenizer:
    tokenizer_mode: linear # tokenizer mode (usually linear or conv_net)
    detokenizer_mode: linear # detokenizer mode (usually linear or conv_net)
    tokenizer_overlap: 2 # pixels overlap between patches, only used for linear tokenizer
    detokenizer_overlap: 2 # pixels overlap between patches, only used for linear detokenizer
    tokenizer_net_channels: null # number of channels in the tokenizer conv_net
    detokenizer_net_channels: null # number of channels in the detokenizer conv_net

training:
  seed: 42
  batch_size: 192
  epochs: 10
  num_workers: 8
  grad_clip: 1.0 # gradient clipping
  optimizer:
    # name: Adam
    # betas: [0.9, 0.999]
    # learning_rate: 0.0001
    # weight_decay: 0.0000
    name: AdamW
    learning_rate: 1e-4
    weight_decay: 0.01
    betas: [0.9, 0.999]
    # name: DAdaptAdam
    # betas: [0.9, 0.999]
    # weight_decay: 0.01
    # name: DAdaptAdamW
    # betas: [0.9, 0.999]
    # weight_decay: 0.1
  criterion: NMSE # MSE or NMSE or MAE

  lr_scheduler:
    schedulers:
      LinearLR: # linear warmup scheduler
        start_factor: 0.001
        end_factor: 1.0
        epochs: 1 # number of training epochs for linear warmup
      # CosineAnnealingWarmRestarts: # cosine annealing scheduler
      #   T_0: 9 # number epochs for one cosine annealing cycle
      #   T_mult: 2 # multiplicative factor for the number of epochs in each cycle
      #   min_lr: 0.0001
      CosineAnnealingLR: # cosine annealing scheduler
        T_max: 9 # Number of epochs for one cosine annealing cycle
        min_lr: 1e-6

data:
  max_samples_per_ds: null # limit the number of samples per epoch, set to null for no limit
  dt_stride: 1 # take every dt_stride-th timestep, if list, randomize between the values
  n_steps_input: 16 # number of input timesteps
  n_steps_output: 1 # number of target timesteps
  out_shape: [128, 64] # output shape (usually)

  data_dir: /Users/zsa8rk/Coding/Large-Physics-Foundation-Model/data/datasets
  datasets:
    # - cylinder_sym_flow_water
    # - cylinder_pipe_flow_water
    # - object_periodic_flow_water
    # - object_sym_flow_water
    # - object_sym_flow_air

    # - heated_object_pipe_flow_air
    - cooled_object_pipe_flow_air

    # - turbulent_radiative_layer_2D
    # - rayleigh_benard
    # - shear_flow
    # - euler
