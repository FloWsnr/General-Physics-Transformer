wandb:
  project: Large-Physics-Foundation-Model
  entity: wsnr-florian
  id: ti-
  tags:
    - GPT-S
    - AllDatasets
    - MainRun
  log_model: gradients # log :all", "parameters" or "gradients" or null
  notes: "T"

logging:
  log_dir: /scratch/zsa8rk/logs
  log_file: null #log.txt
  log_level: INFO
  subdir_name: val_ # subdir name each cycle, should end with an underscore

model:  
  transformer:
    input_channels: 5 # number of input fields (usually 5 (pressure, density, temperature, vel_x, vel_y))
    # number of hidden channels across all layers (must be divisible by 6 and by num_heads)
    model_size: GPT_S # model config (GPT_S, GPT_M, GPT_L, GPT_XL)
    att_mode: full # attention mode (full, full_causal)
    dropout: 0.0 # dropout rate
    pos_enc_mode: absolute # positional encoding mode (usually rope or absolute)
    patch_size: [1, 16, 16] # patch size (usually 4x16x16)
    stochastic_depth_rate: 0.0 # stochastic depth rate
    use_derivatives: true # whether to use derivatives in the model
    integrator: Euler # integrator to use (Euler, RK4, Heun, null)
  tokenizer:
    tokenizer_mode: conv_net # tokenizer mode (usually linear or conv_net, or vqvae)
    detokenizer_mode: conv_net # detokenizer mode (usually linear or conv_net, or vqvae)
    tokenizer_overlap: 0 # pixels overlap between patches, only used for linear tokenizer
    detokenizer_overlap: 0 # pixels overlap between patches, only used for linear detokenizer
    tokenizer_net_channels: [128, 128, 128] # number of channels in the tokenizer conv_net
    detokenizer_net_channels: [128, 128, 128] # number of channels in the detokenizer conv_net

training:
  compile: true
  tf32: true # use tf32 for faster matrix multiplications
  amp: true # use automatic mixed precision training
  mem_budget: 1 # memory budget in percent of total memory. # if below 1, use gradient checkpointing
  seed: 42
  batch_size: 64 # batch size per GPU
  batches: 600e3 # number of batches to use for training

  checkpoint_every_batches: 1e3 # number of batches between checkpoints
  ################################################################
  ########### Validation parameters ############################
  ################################################################
  val_every_batches: 5e3 # number of batches between validation runs
  
  # This is per dataset, also per dt, if separate_dt is true 
  val_frac_samples: 0.2 # fraction of samples to validate on (1 = all val samples)


  num_workers: 16 # number of workers for dataloader per GPU
  prefetch_factor: 2 # how many batches to prefetch for each worker
  grad_clip: 1.0 # gradient clipping, use null for no gradient clipping
  optimizer:
    name: AdamW # optimizer name (Adam, AdamW)
    learning_rate: 1e-4
    weight_decay: 0.01
    betas: [0.9, 0.999]
    # name: Prodigy
    # learning_rate: 1
    # weight_decay: 0.001
    # slice_p: 1
    # betas: [0.9, 0.999]
  criterion: MSE # MSE or MAE

  lr_scheduler:
    first_stage:
      name: LinearLR # linear warmup scheduler
      start_factor: 0.001
      end_factor: 1.0
      num_updates: 5000 # number of updates for linear warmup
    # first_stage:
    #   name: CosineAnnealingLR # cosine annealing scheduler
    #   num_updates: -1 # num batches for cosine annealing, -1 means use all remaining batches
    #   end_factor: 0.01 # percentage of initial learning rate to use as minimum learning rate
      # name: LinearLR # linear 
      # end_factor: 0.0001
      # num_updates: -1 # number of batches for for second stage, -1 means use all remaining batches
    # third_stage:
    #   name: LinearLR # linear cool down scheduler
    #   end_factor: 0
    #   num_updates: 10 # number of batches for linear cool down

data:
  use_normalization: true # whether to normalize the data
  max_samples_per_ds: null # limit the number of samples per epoch, set to null for no limit
  dt_stride: [1,8] # take every dt_stride-th timestep, if list, randomize between the values
  n_steps_input: 4 # number of input timesteps
  n_steps_output: 1 # number of target timesteps
  out_shape: [256, 128] # output shape (usually 256 x 128

  data_dir: data/datasets
  datasets:
    - cylinder_sym_flow_water
    - cylinder_pipe_flow_water
    - object_periodic_flow_water
    - object_sym_flow_water
    - object_sym_flow_air

    - heated_object_pipe_flow_air
    - cooled_object_pipe_flow_air
    - rayleigh_benard_obstacle
    
    - twophase_flow

    - rayleigh_benard
    - shear_flow
    - euler_multi_quadrants_periodicBC
    - acoustic_scattering_inclusions

    # For testing
    # - turbulent_radiative_layer_2D
