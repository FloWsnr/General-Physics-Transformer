{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from einops import rearrange\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hdf5_dataset_supersonic_flow(output_path: Path, data: np.ndarray, mach: float):\n",
    "    \"\"\"\n",
    "    Create HDF5 file with the specified format.\n",
    "    \"\"\"\n",
    "    vel = data[...,-2:]\n",
    "    pressure = data[...,1]\n",
    "    density = data[...,0]\n",
    "\n",
    "    # create a zero array for the temperature\n",
    "    temperature = np.zeros_like(pressure)\n",
    "    \n",
    "    filename = f\"supersonic_flow_Ma_{mach}.hdf5\"\n",
    "\n",
    "    with h5py.File(output_path / filename, \"w\") as f:\n",
    "        # Root attributes\n",
    "        f.attrs[\"simulation_parameters\"] = [\"Ma\"]\n",
    "        f.attrs[\"Ma\"] = mach\n",
    "        f.attrs[\"dataset_name\"] = \"COMSOL_SupersonicFlow\"\n",
    "        f.attrs[\"grid_type\"] = \"cartesian\"\n",
    "        f.attrs[\"n_spatial_dims\"] = 2\n",
    "        f.attrs[\"n_trajectories\"] = data.shape[0]\n",
    "\n",
    "        # Load data from first file to get dimensions\n",
    "        x_coords = np.arange(data.shape[2])\n",
    "        y_coords = np.arange(data.shape[3])\n",
    "        time_steps = np.arange(data.shape[1])\n",
    "\n",
    "        # Create dimensions group\n",
    "        dims = f.create_group(\"dimensions\")\n",
    "        dims.attrs[\"spatial_dims\"] = [\"x\", \"y\"]\n",
    "\n",
    "        time_dset = dims.create_dataset(\"time\", data=time_steps)\n",
    "        time_dset.attrs[\"sample_varying\"] = False\n",
    "\n",
    "        x_dset = dims.create_dataset(\"x\", data=x_coords)\n",
    "        x_dset.attrs[\"sample_varying\"] = False\n",
    "        x_dset.attrs[\"time_varying\"] = False\n",
    "\n",
    "        y_dset = dims.create_dataset(\"y\", data=y_coords)\n",
    "        y_dset.attrs[\"sample_varying\"] = False\n",
    "        y_dset.attrs[\"time_varying\"] = False\n",
    "\n",
    "        # Create boundary conditions group\n",
    "        bc = f.create_group(\"boundary_conditions\")\n",
    "        \n",
    "        x_bc = bc.create_group(\"x_open\")\n",
    "        x_bc.attrs[\"associated_dims\"] = [\"x\"]\n",
    "        x_bc.attrs[\"associated_fields\"] = []\n",
    "        x_bc.attrs[\"bc_type\"] = \"open\"\n",
    "        x_bc.attrs[\"sample_varying\"] = False\n",
    "        x_bc.attrs[\"time_varying\"] = False\n",
    "\n",
    "        mask = np.zeros_like(x_coords, dtype=np.bool)\n",
    "        mask[0] = True\n",
    "        mask[-1] = True\n",
    "        x_bc.create_dataset(\"mask\", data=mask)\n",
    "        x_bc.create_dataset(\"values\", data=np.zeros_like(x_coords))\n",
    "\n",
    "        # y-boundary\n",
    "        y_bc = bc.create_group(\"y_open\")\n",
    "        y_bc.attrs[\"associated_dims\"] = [\"y\"]\n",
    "        y_bc.attrs[\"associated_fields\"] = []\n",
    "        y_bc.attrs[\"bc_type\"] = \"open\"\n",
    "        y_bc.attrs[\"sample_varying\"] = False\n",
    "        y_bc.attrs[\"time_varying\"] = False\n",
    "        mask = np.zeros_like(y_coords, dtype=np.bool)\n",
    "        mask[0] = True\n",
    "        mask[-1] = True\n",
    "        y_bc.create_dataset(\"mask\", data=mask)\n",
    "        y_bc.create_dataset(\"values\", data=np.zeros_like(y_coords))\n",
    "\n",
    "\n",
    "        # Create scalars group\n",
    "        scalars = f.create_group(\"scalars\")\n",
    "        scalars.attrs[\"field_names\"] = [\"Ma\"]\n",
    "\n",
    "        Ma_dset = scalars.create_dataset(\"Ma\", data=mach)\n",
    "        Ma_dset.attrs[\"sample_varying\"] = False\n",
    "        Ma_dset.attrs[\"time_varying\"] = False\n",
    "\n",
    "        # Create t0_fields group for pressure\n",
    "        t0_fields = f.create_group(\"t0_fields\")\n",
    "        t0_fields.attrs[\"field_names\"] = [\"pressure\",\"density\",\"temperature\"]\n",
    "\n",
    "        # Load and store pressure field\n",
    "        pressure_dset = t0_fields.create_dataset(\n",
    "            \"pressure\", data=pressure\n",
    "        )\n",
    "        pressure_dset.attrs[\"dim_varying\"] = [True, True]\n",
    "        pressure_dset.attrs[\"sample_varying\"] = True\n",
    "        pressure_dset.attrs[\"time_varying\"] = True\n",
    "\n",
    "        # Load and store density field\n",
    "        density_dset = t0_fields.create_dataset(\n",
    "            \"density\", data=density\n",
    "        )\n",
    "        density_dset.attrs[\"dim_varying\"] = [True, True]\n",
    "        density_dset.attrs[\"sample_varying\"] = True\n",
    "        density_dset.attrs[\"time_varying\"] = True\n",
    "\n",
    "        # Load and store temperature field\n",
    "        temperature_dset = t0_fields.create_dataset(\n",
    "            \"temperature\", data=temperature\n",
    "        )\n",
    "        temperature_dset.attrs[\"dim_varying\"] = [True, True]\n",
    "        temperature_dset.attrs[\"sample_varying\"] = True\n",
    "        temperature_dset.attrs[\"time_varying\"] = True\n",
    "    \n",
    "        # Create t1_fields group for velocities\n",
    "        t1_fields = f.create_group(\"t1_fields\")\n",
    "        t1_fields.attrs[\"field_names\"] = [\"velocity\"]\n",
    "\n",
    "        # Load velocity components\n",
    "        velocity_dset = t1_fields.create_dataset(\n",
    "            \"velocity\", data=vel\n",
    "        )\n",
    "        velocity_dset.attrs[\"dim_varying\"] = [True, True]\n",
    "        velocity_dset.attrs[\"sample_varying\"] = True\n",
    "        velocity_dset.attrs[\"time_varying\"] = True\n",
    "\n",
    "        # Create empty t2_fields group\n",
    "        t2_fields = f.create_group(\"t2_fields\")\n",
    "        t2_fields.attrs[\"field_names\"] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swap x and y\n",
    "def swap_x_y(data):\n",
    "    data = data.swapaxes(1, 2)\n",
    "    return data\n",
    "\n",
    "\n",
    "def interpolate_data(data):\n",
    "    data = torch.from_numpy(data)\n",
    "    data_r = rearrange(data, \"traj t h w c -> (traj t) c h w\")\n",
    "    interpolated_data = torch.nn.functional.interpolate(\n",
    "        data_r, size=(256,128), mode=\"bicubic\", align_corners=False\n",
    "    )\n",
    "    interpolated_data = rearrange(interpolated_data, \"(traj t) c h w -> traj t h w c\", traj=data.shape[0])\n",
    "    return interpolated_data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all numpy arrays\n",
    "\n",
    "base_path = Path(\"/scratch/zsa8rk/datasets/supersonic_flow\")\n",
    "data_path = base_path / \"data\"\n",
    "data_path.mkdir(exist_ok=True)\n",
    "for file in base_path.glob(\"np/*.npy\"):\n",
    "    ma = float(file.stem[5:8])\n",
    "    data = np.load(file)\n",
    "    data = swap_x_y(data)\n",
    "    # add traj axis to the front\n",
    "    data = data[np.newaxis, ...]\n",
    "    data = interpolate_data(data)\n",
    "    create_hdf5_dataset_supersonic_flow(data_path, data, ma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 39 files into:\n",
      "Train: 31 files\n",
      "Validation: 3 files\n",
      "Test: 5 files\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "# data_path = Path(\"C:/Users/zsa8rk/Coding/Large-Physics-Foundation-Model/data/datasets/cooled_object_pipe_flow_air/data\")\n",
    "def split_datasets(data_path: Path, train_ratio: float = 0.8, val_ratio: float = 0.1, test_ratio: float = 0.1):\n",
    "    \"\"\"Split hdf5 files into train/val/test directories.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path : Path\n",
    "        Path to directory containing hdf5 files\n",
    "    train_ratio : float, optional\n",
    "        Ratio of files to use for training, by default 0.8\n",
    "    val_ratio : float, optional\n",
    "        Ratio of files to use for validation, by default 0.1\n",
    "    test_ratio : float, optional\n",
    "        Ratio of files to use for testing, by default 0.1\n",
    "    \"\"\"\n",
    "    # Create subdirectories\n",
    "    train_dir = data_path / \"train\"\n",
    "    val_dir = data_path / \"valid\" \n",
    "    test_dir = data_path / \"test\"\n",
    "    \n",
    "    for dir in [train_dir, val_dir, test_dir]:\n",
    "        dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Get list of hdf5 files\n",
    "    hdf5_files = list(data_path.glob(\"*.hdf5\"))\n",
    "    \n",
    "    # Shuffle files\n",
    "    random.shuffle(hdf5_files)\n",
    "    \n",
    "    # Calculate split indices\n",
    "    n_files = len(hdf5_files)\n",
    "    n_train = int(n_files * train_ratio)\n",
    "    n_val = int(n_files * val_ratio)\n",
    "    \n",
    "    # Split files\n",
    "    train_files = hdf5_files[:n_train]\n",
    "    val_files = hdf5_files[n_train:n_train + n_val]\n",
    "    test_files = hdf5_files[n_train + n_val:]\n",
    "    \n",
    "    # Move files to respective directories\n",
    "    for file in train_files:\n",
    "        shutil.move(file, train_dir / file.name)\n",
    "    \n",
    "    for file in val_files:\n",
    "        shutil.move(file, val_dir / file.name)\n",
    "        \n",
    "    for file in test_files:\n",
    "        shutil.move(file, test_dir / file.name)\n",
    "    print(f\"Split {n_files} files into:\")\n",
    "    print(f\"Train: {len(train_files)} files\")\n",
    "    print(f\"Validation: {len(val_files)} files\") \n",
    "    print(f\"Test: {len(test_files)} files\")\n",
    "\n",
    "# Split the datasets\n",
    "split_datasets(base_path / \"data\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
